{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ed1af77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LegendreDecomposition:\n",
    "    \"\"\"\n",
    "    Legendre Decomposition for Tensors using the Natural Gradient method.\n",
    "\n",
    "    This class implements the algorithm described in the paper \"Legendre\n",
    "    Decomposition for Tensors\" (arXiv:1802.04502v2).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_iter: int = 20, tol: float = 1e-7):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            n_iter (int): Maximum number of iterations.\n",
    "            tol (float): Tolerance for the norm of the gradient to determine convergence.\n",
    "        \"\"\"\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        self.P_norm_ = None\n",
    "        self.Q_ = None\n",
    "        self.theta_ = None\n",
    "        self.basis_ = None\n",
    "        self.eta_hat_ = None\n",
    "        self.zeta_matrix_ = None\n",
    "        self.omega_ = None\n",
    "        self.shape_ = None\n",
    "        self.rmse_history_ = []\n",
    "\n",
    "    def _precompute_zeta_matrix(self):\n",
    "        \"\"\"\n",
    "        Precomputes the matrix representing the partial order relationship.\n",
    "        \n",
    "        zeta_matrix[i, j] = 1 if basis[i] <= omega[j], and 0 otherwise.\n",
    "        This matrix is central to both decoding (theta -> Q) and encoding (Q -> eta).\n",
    "        \"\"\"\n",
    "        num_basis = len(self.basis_)\n",
    "        num_omega = len(self.omega_)\n",
    "        \n",
    "        # Expand basis and omega for vectorized comparison\n",
    "        basis_expanded = np.array(self.basis_)[:, np.newaxis, :] # Shape: (num_basis, 1, n_dims)\n",
    "        omega_expanded = np.array(self.omega_)[np.newaxis, :, :] # Shape: (1, num_omega, n_dims)\n",
    "\n",
    "        # ζ(u,v) = 1 if u <= v. u is from basis, v is from omega.\n",
    "        # This checks if for every dimension, basis_coord <= omega_coord.\n",
    "        # The result is a boolean matrix of shape (num_basis, num_omega).\n",
    "        le_matrix = np.all(basis_expanded <= omega_expanded, axis=2)\n",
    "        \n",
    "        self.zeta_matrix_ = le_matrix.astype(int)\n",
    "\n",
    "    def fit(self, P: np.ndarray, basis: list[tuple]):\n",
    "        \"\"\"\n",
    "        Decomposes the input tensor P using the given basis.\n",
    "\n",
    "        Args:\n",
    "            P (np.ndarray): The non-negative input tensor.\n",
    "            basis (list[tuple]): A list of index tuples representing the basis B.\n",
    "                                 e.g., [(0, 1), (1, 0)] for a 2D tensor.\n",
    "        \"\"\"\n",
    "        # --- 0. Initialization and Pre-computation ---\n",
    "        self.shape_ = P.shape\n",
    "        self.basis_ = basis\n",
    "        \n",
    "        # The sample space Ω is the set of all indices\n",
    "        self.omega_ = [idx for idx, _ in np.ndenumerate(P)]\n",
    "        \n",
    "        # Normalize the input tensor P to make it a probability distribution\n",
    "        p_sum = np.sum(P)\n",
    "        if p_sum == 0:\n",
    "            raise ValueError(\"Input tensor P cannot be all zeros.\")\n",
    "        self.P_norm_ = P / p_sum\n",
    "        \n",
    "        p_flat = self.P_norm_.flatten()\n",
    "        \n",
    "        # This is the most important pre-computation step\n",
    "        self._precompute_zeta_matrix()\n",
    "        \n",
    "        # Initialize parameters θ (theta)\n",
    "        self.theta_ = np.zeros(len(self.basis_))\n",
    "        \n",
    "        # Compute target expectation η̂ (eta-hat) from P. This is done only once.\n",
    "        # η̂_v = Σ_{u∈Ω, v≤u} p_u  (Eq. 3 with P)\n",
    "        self.eta_hat_ = self.zeta_matrix_ @ p_flat\n",
    "        \n",
    "        self.rmse_history_ = []\n",
    "\n",
    "        print(f\"Starting optimization for tensor of shape {self.shape_} with {len(self.basis_)} basis elements.\")\n",
    "\n",
    "        # --- Main Optimization Loop (Natural Gradient) ---\n",
    "        for i in range(self.n_iter):\n",
    "            # --- 1. Decode: Compute Q from current θ (theta) ---\n",
    "            # log q_v = Σ_{u∈B, u≤v} θ_u - ψ(θ) (Eq. 2)\n",
    "            # The sum part is a matrix-vector product: ζ^T * θ\n",
    "            log_q_unnormalized = self.zeta_matrix_.T @ self.theta_\n",
    "            \n",
    "            # Use LogSumExp trick for numerical stability (part of softmax)\n",
    "            log_q_unnormalized -= np.max(log_q_unnormalized)\n",
    "            q_temp = np.exp(log_q_unnormalized)\n",
    "            q_flat = q_temp / np.sum(q_temp)\n",
    "            \n",
    "            # --- 2. Encode: Compute η (eta) from Q ---\n",
    "            # η_v = Σ_{u∈Ω, v≤u} q_u (Eq. 3)\n",
    "            # This is a matrix-vector product: ζ * q\n",
    "            eta = self.zeta_matrix_ @ q_flat\n",
    "            \n",
    "            # --- 3. Compute Gradient Δη ---\n",
    "            delta_eta = eta - self.eta_hat_\n",
    "            \n",
    "            # --- 4. Check for Convergence ---\n",
    "            grad_norm = np.linalg.norm(delta_eta)\n",
    "            self.Q_ = q_flat.reshape(self.shape_) * p_sum # Rescale to original magnitude\n",
    "            rmse = np.sqrt(np.mean((P - self.Q_)**2))\n",
    "            self.rmse_history_.append(rmse)\n",
    "            \n",
    "            print(f\"Iter {i+1:2d}: Grad Norm = {grad_norm:.4e}, RMSE = {rmse:.4f}\")\n",
    "            if grad_norm < self.tol:\n",
    "                print(\"Convergence reached.\")\n",
    "                break\n",
    "\n",
    "            # --- 5. Compute Fisher Information Matrix G ---\n",
    "            # G_uv = Cov(ζ_u, ζ_v) = Σ_w q_w ζ(u,w)ζ(v,w) - η_uη_v (Eq. 5)\n",
    "            # Vectorized form: G = ζ * diag(q) * ζ^T - η * η^T\n",
    "            G = (self.zeta_matrix_ * q_flat) @ self.zeta_matrix_.T - np.outer(eta, eta)\n",
    "\n",
    "            # --- 6. Update θ (theta) ---\n",
    "            # θ_next = θ - G⁻¹ * Δη\n",
    "            # Use np.linalg.solve for better stability and performance\n",
    "            try:\n",
    "                # Add a small regularization term (ridge) for stability\n",
    "                reg = 1e-8 * np.eye(G.shape[0])\n",
    "                update_step = np.linalg.solve(G + reg, delta_eta)\n",
    "                self.theta_ -= update_step\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(\"Warning: Fisher matrix is singular. Using pseudo-inverse.\")\n",
    "                update_step = np.linalg.pinv(G) @ delta_eta\n",
    "                self.theta_ -= update_step\n",
    "                \n",
    "        # Final reconstructed tensor\n",
    "        self.Q_ = q_flat.reshape(self.shape_) * p_sum\n",
    "        \n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ce5a9ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization for tensor of shape (4, 5) with 8 basis elements.\n",
      "Iter  1: Grad Norm = 2.6654e-01, RMSE = 4.9787\n",
      "Iter  2: Grad Norm = 3.5190e-02, RMSE = 3.4138\n",
      "Iter  3: Grad Norm = 3.1348e-03, RMSE = 3.3733\n",
      "Iter  4: Grad Norm = 4.1308e-05, RMSE = 3.3720\n",
      "Iter  5: Grad Norm = 6.8692e-09, RMSE = 3.3719\n",
      "Iter  6: Grad Norm = 8.4791e-16, RMSE = 3.3719\n",
      "Convergence reached.\n",
      "\n",
      "--- Results ---\n",
      "Original Tensor P:\n",
      " [[10 12  5  2  1]\n",
      " [15 20  8  3  2]\n",
      " [ 8 10 15  9  4]\n",
      " [ 3  4 11  8  5]]\n",
      "\n",
      "Reconstructed Tensor Q:\n",
      " [[ 6.97  8.9   7.55  4.26  2.32]\n",
      " [11.15 14.25 12.08  6.81  3.72]\n",
      " [10.68 13.65 11.57  6.53  3.56]\n",
      " [ 7.2   9.2   7.8   4.4   2.4 ]]\n",
      "\n",
      "Final RMSE: 3.3719\n"
     ]
    }
   ],
   "source": [
    "# 1. Create a synthetic 2D tensor (a 4x5 matrix)\n",
    "P = np.array([\n",
    "    [10, 12, 5, 2, 1],\n",
    "    [15, 20, 8, 3, 2],\n",
    "    [8,  10, 15, 9, 4],\n",
    "    [3,  4,  11, 8, 5]\n",
    "])\n",
    "\n",
    "# 2. Define a basis B. \n",
    "# Let's choose a basis that only includes the first row and first column.\n",
    "# This corresponds to a low-rank approximation in the log-domain.\n",
    "# Note: Using 0-based indexing.\n",
    "basis_elements = [(i, 0) for i in range(P.shape[0])] + \\\n",
    "                    [(0, j) for j in range(1, P.shape[1])] # Avoid adding (0,0) twice\n",
    "\n",
    "# 3. Create a model and fit it to the data\n",
    "ld_model = LegendreDecomposition(n_iter=100, tol=1e-10)\n",
    "ld_model.fit(P, basis=basis_elements)\n",
    "\n",
    "# 4. Get the reconstructed tensor\n",
    "Q = ld_model.Q_\n",
    "\n",
    "final_rmse = np.sqrt(np.mean((P - Q)**2))\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\"Original Tensor P:\\n\", np.round(P, 2))\n",
    "print(\"\\nReconstructed Tensor Q:\\n\", np.round(Q, 2))\n",
    "print(f\"\\nFinal RMSE: {final_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08c06817",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting optimization for tensor of shape (4, 5) with 14 basis elements.\n",
      "Iter  1: Grad Norm = 2.7654e-01, RMSE = 4.9787\n",
      "Iter  2: Grad Norm = 1.1325e-01, RMSE = 2.6085\n",
      "Iter  3: Grad Norm = 1.0684e-02, RMSE = 0.5113\n",
      "Iter  4: Grad Norm = 1.9029e-04, RMSE = 0.4285\n",
      "Iter  5: Grad Norm = 1.0094e-07, RMSE = 0.4285\n",
      "Iter  6: Grad Norm = 6.2807e-14, RMSE = 0.4285\n",
      "Convergence reached.\n",
      "\n",
      "--- Results ---\n",
      "Original Tensor P:\n",
      " [[10 12  5  2  1]\n",
      " [15 20  8  3  2]\n",
      " [ 8 10 15  9  4]\n",
      " [ 3  4 11  8  5]]\n",
      "\n",
      "Reconstructed Tensor Q:\n",
      " [[10.   12.    4.95  1.97  1.08]\n",
      " [15.   20.    8.05  3.2   1.75]\n",
      " [ 7.92 10.08 14.    9.06  4.94]\n",
      " [ 3.08  3.92 12.    7.76  4.24]]\n",
      "\n",
      "Final RMSE: 0.4285\n"
     ]
    }
   ],
   "source": [
    "# 例：値が大きい上位10個のインデックスを基底に追加する\n",
    "\n",
    "# テンソルの各要素とそのインデックスを平坦化\n",
    "flat_P = P.flatten()\n",
    "indices = [idx for idx, _ in np.ndenumerate(P)]\n",
    "\n",
    "# 値の大きい順にソート\n",
    "sorted_indices = sorted(zip(flat_P, indices), key=lambda x: x[0], reverse=True)\n",
    "\n",
    "# 上位10個のインデックスを取得\n",
    "top_10_indices = [idx for val, idx in sorted_indices[:10]]\n",
    "\n",
    "# これを基底に追加する\n",
    "new_basis = list(set(basis_elements + top_10_indices))\n",
    "\n",
    "# 3. Create a model and fit it to the data\n",
    "ld_model = LegendreDecomposition(n_iter=100, tol=1e-10)\n",
    "ld_model.fit(P, basis=new_basis)\n",
    "\n",
    "# 4. Get the reconstructed tensor\n",
    "Q = ld_model.Q_\n",
    "\n",
    "final_rmse = np.sqrt(np.mean((P - Q)**2))\n",
    "\n",
    "print(\"\\n--- Results ---\")\n",
    "print(\"Original Tensor P:\\n\", np.round(P, 2))\n",
    "print(\"\\nReconstructed Tensor Q:\\n\", np.round(Q, 2))\n",
    "print(f\"\\nFinal RMSE: {final_rmse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178fc38d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c3c502d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
