{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e958129d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Sequential sampling (2 chains in 1 job)\n",
      "NUTS: [ls, eta, f_rotated_, f_pred]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7385c73ece074d6f9e7a334353e31234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 2 chains for 100 tune and 100 draw iterations (200 + 200 draws total) took 257 seconds.\n",
      "Chain 0 reached the maximum tree depth. Increase `max_treedepth`, increase `target_accept` or reparameterize.\n",
      "We recommend running at least 4 chains for robust computation of convergence diagnostics\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Sampling: [f_pred]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "122c3c1fe0b44dd59abf3f5d803fffc3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'f_pred'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 82\u001b[39m\n\u001b[32m     79\u001b[39m     ppc = pm.sample_posterior_predictive(trace, var_names=[\u001b[33m\"\u001b[39m\u001b[33mf_pred\u001b[39m\u001b[33m\"\u001b[39m], random_seed=\u001b[32m42\u001b[39m)\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Extract latent predictive samples (shape: n_samples x n_points)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m82\u001b[39m f_pred_samples = \u001b[43mppc\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf_pred\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     84\u001b[39m \u001b[38;5;66;03m# Convert latent draws to probabilities via normal cdf (probit)\u001b[39;00m\n\u001b[32m     85\u001b[39m prob_pred_samples = norm.cdf(f_pred_samples)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/ws/sandbox/.venv-sandbox/lib/python3.12/site-packages/arviz/data/inference_data.py:271\u001b[39m, in \u001b[36mInferenceData.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Get item by key.\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._groups_all:\n\u001b[32m--> \u001b[39m\u001b[32m271\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[32m    272\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, key)\n",
      "\u001b[31mKeyError\u001b[39m: 'f_pred'"
     ]
    }
   ],
   "source": [
    "# Gaussian Process classification with a probit likelihood (PyMC)\n",
    "# All comments in this script are in English as requested.\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pymc as pm\n",
    "import arviz as az\n",
    "from scipy.stats import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# For more informative PyTensor tracebacks during debugging (optional)\n",
    "import pytensor\n",
    "pytensor.config.exception_verbosity = \"high\"\n",
    "\n",
    "# Fix random seed for reproducibility\n",
    "RNG = np.random.default_rng(42)\n",
    "\n",
    "# Generate a simple 1D toy dataset for binary classification\n",
    "n_train = 50\n",
    "X_train = np.linspace(-3, 3, n_train)[:, None]\n",
    "\n",
    "# True latent function (for data generation only)\n",
    "def true_latent(x):\n",
    "    return np.sin(1.5 * x).ravel()\n",
    "\n",
    "f_true = true_latent(X_train.ravel())\n",
    "\n",
    "# Convert latent values to probabilities via the standard normal CDF (probit)\n",
    "Phi_np = lambda z: norm.cdf(z)\n",
    "prob = Phi_np(f_true)\n",
    "\n",
    "# Sample binary labels using the probit probabilities\n",
    "y_train = RNG.binomial(1, prob)\n",
    "\n",
    "# Check no NaNs/Infs in the data\n",
    "assert np.isfinite(X_train).all(), \"X_train contains NaN or Inf\"\n",
    "assert np.isfinite(y_train).all(), \"y_train contains NaN or Inf\"\n",
    "\n",
    "# Prepare test points for prediction\n",
    "X_test = np.linspace(-4, 4, 200)[:, None]\n",
    "\n",
    "# Build the PyMC model with numerical-stability improvements\n",
    "with pm.Model() as gp_probit_model:\n",
    "    # Narrower, more stable hyperpriors to avoid extremes\n",
    "    ls = pm.Gamma(\"ls\", alpha=3.0, beta=1.0)   # lengthscale\n",
    "    eta = pm.HalfNormal(\"eta\", sigma=0.5)      # amplitude\n",
    "\n",
    "    # Kernel: RBF (ExpQuad) plus a tiny white noise term for numerical stability\n",
    "    # The WhiteNoise term adds a small diagonal jitter to the covariance matrix.\n",
    "    cov = eta**2 * pm.gp.cov.ExpQuad(input_dim=1, ls=ls) + pm.gp.cov.WhiteNoise(1e-6)\n",
    "\n",
    "    # Latent GP prior\n",
    "    gp = pm.gp.Latent(cov_func=cov)\n",
    "    f = gp.prior(\"f\", X=X_train)\n",
    "\n",
    "    # Probit link: transform latent f to probability using normal CDF\n",
    "    p = 0.5 * (1 + pm.math.erf(f / pm.math.sqrt(2)))\n",
    "\n",
    "    # Bernoulli likelihood\n",
    "    y = pm.Bernoulli(\"y\", p=p, observed=y_train)\n",
    "\n",
    "    # Conditional GP for prediction: add explicit jitter here as well\n",
    "    f_pred = gp.conditional(\"f_pred\", X_test, jitter=1e-6)\n",
    "\n",
    "    # Inference: sample from the posterior.\n",
    "    # Use cores=1 to avoid multiprocessing issues (safe in notebooks).\n",
    "    # If you run in a robust terminal environment and want parallel chains, set cores>1.\n",
    "    trace = pm.sample(\n",
    "        draws=100,\n",
    "        tune=100,\n",
    "        chains=2,\n",
    "        cores=1,                   # <-- run single-core to avoid EOFError from multiprocessing\n",
    "        target_accept=0.8,\n",
    "        return_inferencedata=True,\n",
    "        random_seed=42,\n",
    "    )\n",
    "\n",
    "    # Draw posterior predictive latent samples for f_pred directly using pm.draw\n",
    "    f_pred_samples = pm.draw(f_pred, draws=500, random_seed=42)\n",
    "\n",
    "\n",
    "# Extract latent predictive samples (shape: n_samples x n_points)\n",
    "f_pred_samples = ppc[\"f_pred\"]\n",
    "\n",
    "# Convert latent draws to probabilities via normal cdf (probit)\n",
    "prob_pred_samples = norm.cdf(f_pred_samples)\n",
    "\n",
    "# Posterior mean probability and 95% credible interval\n",
    "prob_mean = prob_pred_samples.mean(axis=0)\n",
    "prob_hpd = az.hdi(prob_pred_samples, hdi_prob=0.95)\n",
    "\n",
    "# Plot results\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.scatter(X_train.ravel(), y_train - 0.03 + 0.06 * RNG.random(n_train),\n",
    "            c=y_train, cmap=\"bwr\", label=\"training labels\")\n",
    "plt.plot(X_test.ravel(), prob_mean, label=\"posterior mean prob\", lw=2)\n",
    "plt.fill_between(X_test.ravel(), prob_hpd[:, 0], prob_hpd[:, 1], alpha=0.3, label=\"95% credible interval\")\n",
    "plt.ylim(-0.1, 1.1)\n",
    "plt.xlabel(\"X\")\n",
    "plt.ylabel(\"Prob(y=1)\")\n",
    "plt.title(\"Gaussian Process Classification with Probit Likelihood (PyMC)\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Print a brief summary of hyperparameters posterior\n",
    "print(az.summary(trace, var_names=[\"ls\", \"eta\"]))\n",
    "\n",
    "# Approximate training accuracy: find nearest test indices for train points\n",
    "idx_closest = np.argmin(np.abs(X_test.ravel()[None, :] - X_train.ravel()[:, None]), axis=1)\n",
    "train_preds_at_trainloc = (prob_mean[idx_closest] >= 0.5).astype(int)\n",
    "acc = accuracy_score(y_train, train_preds_at_trainloc)\n",
    "print(f\"Approximate training accuracy (at nearest test points): {acc:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6892eb53",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60378c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf5b218",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-sandbox (3.12.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
