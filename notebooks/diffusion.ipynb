{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f752d160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================================================\n",
    "#      Numerical Experiment for Operator-Theoretic Error Analysis\n",
    "# ==============================================================================\n",
    "#\n",
    "# Objective:\n",
    "# To empirically validate the theory that the local discretization error in\n",
    "# diffusion model sampling correlates with a proxy for the Koopman operator norm,\n",
    "# which is tied to the Lipschitz properties of the learned score network.\n",
    "#\n",
    "# Steps:\n",
    "# 1. Train two score-based models on a 2D toy dataset:\n",
    "#    - Model A: Standard training.\n",
    "#    - Model B: Training with regularization to enforce smoothness.\n",
    "# 2. For each model, compute two metrics across time t:\n",
    "#    - Metric 1 (Theory): A proxy for the operator norm (gradient norm of the score).\n",
    "#    - Metric 2 (Empirical): The actual local discretization error.\n",
    "# 3. Plot the results to show the correlation between the two metrics.\n",
    "#\n",
    "# ==============================================================================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.datasets import make_moons\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# --- 1. Setup and Hyperparameters ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Data parameters\n",
    "n_samples = 5000\n",
    "batch_size = 256\n",
    "\n",
    "# Diffusion parameters\n",
    "T_end = 1.0\n",
    "beta_0 = 0.0001\n",
    "beta_T = 0.02\n",
    "\n",
    "# Model and Training parameters\n",
    "n_epochs = 2000\n",
    "lr = 1e-4\n",
    "lambda_reg = 0.01 # Regularization strength for Model B\n",
    "\n",
    "# Analysis parameters\n",
    "timesteps_to_eval = np.linspace(1e-5, T_end, 20)\n",
    "n_error_samples = 1000 # Number of samples to estimate error\n",
    "\n",
    "# --- 2. Diffusion Process Helper Functions (VP-SDE) ---\n",
    "\n",
    "def beta_t(t):\n",
    "    return beta_0 + t * (beta_T - beta_0)\n",
    "\n",
    "def alpha_t(t):\n",
    "    log_alpha = -0.25 * t**2 * (beta_T - beta_0) - 0.5 * t * beta_0\n",
    "    return torch.exp(log_alpha)\n",
    "\n",
    "def sigma_t(t):\n",
    "    return torch.sqrt(1.0 - torch.exp(-0.5 * t**2 * (beta_T - beta_0) - t * beta_0))\n",
    "\n",
    "# --- 3. Score Network Model ---\n",
    "\n",
    "class ScoreNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(3, 128), nn.SiLU(),\n",
    "            nn.Linear(128, 128), nn.SiLU(),\n",
    "            nn.Linear(128, 2)\n",
    "        )\n",
    "        # Time embedding\n",
    "        self.t_embed = nn.Sequential(nn.Linear(1, 128), nn.SiLU(), nn.Linear(128, 128))\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        # Concatenate x and time embedding\n",
    "        t_embedding = self.t_embed(t.view(-1, 1))\n",
    "        x_with_time = torch.cat([x, t.view(-1, 1)], dim=1)\n",
    "        return self.net(x_with_time)\n",
    "\n",
    "# --- 4. Training Function ---\n",
    "\n",
    "def train_model(is_regularized=False):\n",
    "    print(f\"--- Training Model {'B (Regularized)' if is_regularized else 'A (Standard)'} ---\")\n",
    "    score_net = ScoreNet().to(device)\n",
    "    optimizer = optim.Adam(score_net.parameters(), lr=lr)\n",
    "    \n",
    "    # Create dataset\n",
    "    X, _ = make_moons(n_samples=n_samples, noise=0.05)\n",
    "    dataset = TensorDataset(torch.from_numpy(X).float())\n",
    "    loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    for epoch in tqdm(range(n_epochs), desc=\"Training\"):\n",
    "        for data, in loader:\n",
    "            x0 = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Sample random time t\n",
    "            t = torch.rand(x0.shape[0], device=device) * T_end\n",
    "            \n",
    "            # Perturb data\n",
    "            alpha = alpha_t(t).view(-1, 1)\n",
    "            sigma = sigma_t(t).view(-1, 1)\n",
    "            noise = torch.randn_like(x0)\n",
    "            xt = alpha * x0 + sigma * noise\n",
    "            \n",
    "            # Predict score (which is proportional to -noise/sigma)\n",
    "            predicted_noise = score_net(xt, t)\n",
    "            target_noise = -noise / sigma\n",
    "            \n",
    "            # DSM loss\n",
    "            loss = ((predicted_noise - target_noise)**2).mean()\n",
    "\n",
    "            # --- Regularization for Model B ---\n",
    "            if is_regularized:\n",
    "                xt.requires_grad_(True)\n",
    "                predicted_noise_reg = score_net(xt, t)\n",
    "                \n",
    "                # Compute gradient norm (proxy for Lipschitz)\n",
    "                grad_outputs = torch.ones_like(predicted_noise_reg)\n",
    "                gradients = torch.autograd.grad(\n",
    "                    outputs=predicted_noise_reg,\n",
    "                    inputs=xt,\n",
    "                    grad_outputs=grad_outputs,\n",
    "                    create_graph=True\n",
    "                )[0]\n",
    "                grad_norm = gradients.view(gradients.shape[0], -1).norm(2, dim=1)\n",
    "                reg_loss = (grad_norm**2).mean()\n",
    "                \n",
    "                loss += lambda_reg * reg_loss\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "    \n",
    "    return score_net\n",
    "\n",
    "# --- 5. Metric Calculation Functions ---\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_proxy_lip(score_net, t_val, n_samples=1000):\n",
    "    \"\"\"Calculates the proxy for the operator norm (Metric 1).\"\"\"\n",
    "    t = torch.full((n_samples,), t_val, device=device)\n",
    "    # Sample from the perturbed distribution p_t (approximated)\n",
    "    # We start from prior and denoise for a bit to get a better p_t sample\n",
    "    z = torch.randn(n_samples, 2, device=device)\n",
    "    xt = alpha_t(t).view(-1, 1) * torch.randn_like(z) + sigma_t(t).view(-1, 1) * z # Simple approximation of p_t\n",
    "    \n",
    "    xt.requires_grad_(True)\n",
    "    \n",
    "    predicted_noise = score_net(xt, t)\n",
    "    \n",
    "    grad_outputs = torch.ones_like(predicted_noise)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=predicted_noise,\n",
    "        inputs=xt,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=False\n",
    "    )[0]\n",
    "    \n",
    "    grad_norm = gradients.view(gradients.shape[0], -1).norm(2, dim=1).mean().item()\n",
    "    return grad_norm\n",
    "\n",
    "@torch.no_grad()\n",
    "def calculate_local_error(score_net, t_val, n_samples=1000, dt=0.01, small_steps=10):\n",
    "    \"\"\"Calculates the empirical local discretization error (Metric 2).\"\"\"\n",
    "    t = torch.full((n_samples,), t_val, device=device)\n",
    "    # Get samples at time t\n",
    "    z = torch.randn(n_samples, 2, device=device)\n",
    "    xt = alpha_t(t).view(-1, 1) * torch.randn_like(z) + sigma_t(t).view(-1, 1) * z\n",
    "\n",
    "    # (A) \"Ground Truth\" solution with very small steps\n",
    "    x_true = xt.clone()\n",
    "    dt_small = dt / small_steps\n",
    "    for i in range(small_steps):\n",
    "        t_curr = t - i * dt_small\n",
    "        beta = beta_t(t_curr).view(-1, 1)\n",
    "        score = -score_net(x_true, t_curr) / sigma_t(t_curr).view(-1, 1)\n",
    "        drift = -0.5 * beta * (x_true + 2 * score)\n",
    "        x_true = x_true - drift * dt_small\n",
    "\n",
    "    # (B) Approximated solution with one large step\n",
    "    beta = beta_t(t).view(-1, 1)\n",
    "    score = -score_net(xt, t) / sigma_t(t).view(-1, 1)\n",
    "    drift = -0.5 * beta * (xt + 2 * score)\n",
    "    x_approx = xt - drift * dt\n",
    "    \n",
    "    error = ((x_true - x_approx)**2).mean().item()\n",
    "    return error\n",
    "\n",
    "# --- 6. Main Execution ---\n",
    "\n",
    "# Train the two models\n",
    "model_A = train_model(is_regularized=False)\n",
    "model_B = train_model(is_regularized=True)\n",
    "\n",
    "models = {'Model A (Standard)': model_A, 'Model B (Regularized)': model_B}\n",
    "results = {}\n",
    "\n",
    "# Calculate metrics for each model\n",
    "for name, model in models.items():\n",
    "    print(f\"--- Evaluating {name} ---\")\n",
    "    proxy_lips = []\n",
    "    local_errors = []\n",
    "    for t_val in tqdm(timesteps_to_eval, desc=f\"Evaluating {name}\"):\n",
    "        proxy_lips.append(calculate_proxy_lip(model, t_val))\n",
    "        local_errors.append(calculate_local_error(model, t_val))\n",
    "    results[name] = {'proxy_lip': np.array(proxy_lips), 'error': np.array(local_errors)}\n",
    "\n",
    "# --- 7. Plotting the Results ---\n",
    "\n",
    "plt.style.use('seaborn-whitegrid')\n",
    "fig, axes = plt.subplots(1, 3, figsize=(21, 6))\n",
    "fig.suptitle(\"Operator-Theoretic Analysis of Discretization Error\", fontsize=16)\n",
    "\n",
    "# Plot 1: Proxy for Operator Norm vs. Time\n",
    "ax = axes[0]\n",
    "ax.plot(timesteps_to_eval, results['Model A (Standard)']['proxy_lip'], 'o-', label='Model A (Standard)')\n",
    "ax.plot(timesteps_to_eval, results['Model B (Regularized)']['proxy_lip'], 's-', label='Model B (Regularized)')\n",
    "ax.set_title(\"Result 1: Proxy for Operator Norm vs. Time\")\n",
    "ax.set_xlabel(\"Time (t)\")\n",
    "ax.set_ylabel(\"Proxy for Operator Norm (Score Gradient Norm)\")\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 2: Local Discretization Error vs. Time\n",
    "ax = axes[1]\n",
    "ax.plot(timesteps_to_eval, results['Model A (Standard)']['error'], 'o-', label='Model A (Standard)')\n",
    "ax.plot(timesteps_to_eval, results['Model B (Regularized)']['error'], 's-', label='Model B (Regularized)')\n",
    "ax.set_title(\"Result 2: Local Discretization Error vs. Time\")\n",
    "ax.set_xlabel(\"Time (t)\")\n",
    "ax.set_ylabel(\"Empirical Local Error (MSE)\")\n",
    "ax.legend()\n",
    "ax.set_yscale('log')\n",
    "\n",
    "# Plot 3: Correlation Plot\n",
    "ax = axes[2]\n",
    "ax.plot(results['Model A (Standard)']['proxy_lip'], results['Model A (Standard)']['error'], 'o', label='Model A (Standard)')\n",
    "ax.plot(results['Model B (Regularized)']['proxy_lip'], results['Model B (Regularized)']['error'], 's', label='Model B (Regularized)')\n",
    "ax.set_title(\"Result 3: Correlation between Theory and Practice\")\n",
    "ax.set_xlabel(\"Theoretical Proxy (Score Gradient Norm)\")\n",
    "ax.set_ylabel(\"Empirical Error (MSE)\")\n",
    "ax.legend()\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.show()\n",
    "\n",
    "# Visualize some generated samples\n",
    "@torch.no_grad()\n",
    "def generate_samples(score_net, n_samples=500, n_steps=100):\n",
    "    xt = torch.randn(n_samples, 2, device=device)\n",
    "    ts = np.linspace(T_end, 1e-5, n_steps)\n",
    "    dt = ts[0] - ts[1]\n",
    "    \n",
    "    for t_val in ts:\n",
    "        t = torch.full((n_samples,), t_val, device=device)\n",
    "        beta = beta_t(t).view(-1, 1)\n",
    "        sigma = sigma_t(t).view(-1, 1)\n",
    "        score = -score_net(xt, t) / sigma\n",
    "        drift = -0.5 * beta * (xt + 2 * score)\n",
    "        noise = torch.randn_like(xt) if t_val > 1e-5 else 0\n",
    "        xt = xt - drift * dt + torch.sqrt(beta * dt) * noise\n",
    "    return xt.cpu().numpy()\n",
    "\n",
    "samples_A = generate_samples(model_A)\n",
    "samples_B = generate_samples(model_B)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "axes[0].scatter(samples_A[:, 0], samples_A[:, 1], s=5, alpha=0.5)\n",
    "axes[0].set_title(\"Samples from Model A (Standard)\")\n",
    "axes[0].set_aspect('equal', adjustable='box')\n",
    "axes[1].scatter(samples_B[:, 0], samples_B[:, 1], s=5, alpha=0.5)\n",
    "axes[1].set_title(\"Samples from Model B (Regularized)\")\n",
    "axes[1].set_aspect('equal', adjustable='box')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ad55fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80bf305",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcf477f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b28562b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67043245",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "315409be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7197d867",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
